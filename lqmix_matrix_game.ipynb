{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74665d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      "Greedy Action: 1 2\n",
      "Joint Action Value\n",
      " tensor([[0.4841, 0.6230, 0.5928],\n",
      "        [0.2961, 0.4350, 0.4049],\n",
      "        [0.3867, 0.5256, 0.4954]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# a simple version of LQMIX in one step matrix game\n",
    "# for simplicity, we omit the projection and auxiliary agent netowrk (since there only exists one state)\n",
    "from ntpath import join\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.optim import RMSprop\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from math import *\n",
    "\n",
    "explore=True\n",
    "factorization = \"owqmix\"           # qmix, vdn, owqmix, lqmix\n",
    "\n",
    "# env\n",
    "matrix = th.tensor([[8,-12,-12],[-12,6,6],[-12,6,6]]).float()\n",
    "mat_size = 3\n",
    "episodes = 10000\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "epsl = 1\n",
    "weight = 0.5\n",
    "sigma_threshold = 0.5\n",
    "random_reward = False\n",
    "random_prob = 0.5\n",
    "random_reward_value = [12, 0]\n",
    "\n",
    "Q_matrix = th.randint(-10,10,(mat_size, mat_size)).float()\n",
    "Q_matrix_cal = th.randint(-10,10,(mat_size, mat_size)).float()\n",
    "### lqmix specific\n",
    "visition, h = th.zeros(1, 2, 3).long(), th.zeros(1, 2, 3)\n",
    "\n",
    "# agent\n",
    "class Q_mat(nn.Module):\n",
    "    def __init__(self, mat_size):\n",
    "        super(Q_mat, self).__init__()\n",
    "        self.Q_net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, mat_size))\n",
    "    def forward(self, inp):\n",
    "        return self.Q_net(inp)\n",
    "# qmixer\n",
    "class Mixer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixer, self).__init__()\n",
    "        self.mixer_net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))\n",
    "        self.v_net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 1))\n",
    "    def forward(self, inp, q_1, q_2):\n",
    "        return th.abs(self.mixer_net(inp))[0] * q_1 + th.abs(self.mixer_net(inp))[1] * q_2 + self.v_net(inp)\n",
    "# lqmix specific\n",
    "class RewardPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RewardPredictor, self).__init__()\n",
    "        self.r_net = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, 1))\n",
    "        self.s_net = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, 1))\n",
    "    def forward(self, inputs):\n",
    "        r = self.r_net(inputs)\n",
    "        sigma = self.s_net(inputs)\n",
    "        return r, sigma\n",
    "    \n",
    "def look_up_temp(visition,action):\n",
    "    action = action.unsqueeze(-1)\n",
    "    visit = th.gather(visition[0], dim=-1, index=action).squeeze(-1)\n",
    "    temp = 0.999 ** visit.min(-1)[0].item()\n",
    "    visition[0].scatter_add_(-1, action.long(), th.ones_like(action))\n",
    "    return th.tensor(temp).expand_as(action)\n",
    "\n",
    "def look_up_h(h, action, reward, is_sr, predict_reward, is_greedy):\n",
    "    h_inm = th.gather(h[0], dim=-1, index=action.unsqueeze(-1))\n",
    "    rewards = reward.unsqueeze(-1).unsqueeze(-1).repeat(2,1)\n",
    "    if is_sr and is_greedy:\n",
    "        h1 = predict_reward.expand_as(rewards)\n",
    "    elif is_sr:\n",
    "        h1 = h_inm\n",
    "    else:\n",
    "        h1 = th.where(rewards > h_inm, rewards, h_inm)\n",
    "    h[0].scatter_(-1, action.unsqueeze(-1).long(), h1)\n",
    "    return th.tensor(h1).expand_as(action.unsqueeze(-1))\n",
    "\n",
    "def transfrom_onehot(joint_ac):\n",
    "    joint_ac = joint_ac.unsqueeze(-1)\n",
    "    y_onehot = joint_ac.new(*joint_ac.shape[:-1], 3).zero_()\n",
    "    y_onehot.scatter_(-1, joint_ac.long(), 1)\n",
    "    return y_onehot.view(-1).float()\n",
    "\n",
    "q_in = th.ones(4)\n",
    "agent_1 = Q_mat(mat_size)\n",
    "agent_2 = Q_mat(mat_size)\n",
    "\n",
    "params = list(agent_1.parameters())\n",
    "params += list(agent_2.parameters())\n",
    "if factorization in [\"qmix\",\"owqmix\",\"lqmix\"]:\n",
    "    mixer = Mixer()\n",
    "    params += list(mixer.parameters())\n",
    "if factorization == \"lqmix\":\n",
    "    reward_predictor = RewardPredictor()\n",
    "    params += list(reward_predictor.parameters())\n",
    "\n",
    "optimiser = RMSprop(params=params, lr=lr)\n",
    "\n",
    "for ep in range(episodes):\n",
    "    loss, reward_loss = 0, 0\n",
    "    for i in range(batch_size):\n",
    "        q_out1 = agent_1.forward(q_in)\n",
    "        q_out2 = agent_2.forward(q_in)\n",
    "        rand = random.random()\n",
    "        if rand < epsl:                 # exploration\n",
    "            ac_1 = th.randint(0,mat_size,()).long()\n",
    "            ac_2 = th.randint(0,mat_size,()).long()\n",
    "        else:                           \n",
    "            ac_1 = q_out1.max(0)[1].long()\n",
    "            ac_2 = q_out2.max(0)[1].long()\n",
    "        if random_reward:\n",
    "            if ac_1 == 1 and ac_2 == 1:\n",
    "                if random.random() < random_prob:\n",
    "                    Gt = th.tensor(random_reward_value[0]).float()\n",
    "                else:\n",
    "                    Gt = th.tensor(random_reward_value[1]).float()\n",
    "            else:\n",
    "                Gt = matrix[ac_1, ac_2]\n",
    "        else:\n",
    "            Gt = matrix[ac_1, ac_2]\n",
    "        if factorization in [\"qmix\",\"owqmix\",\"lqmix\"]:\n",
    "            q_tot = mixer(q_in, q_out1[ac_1], q_out2[ac_2])\n",
    "        elif factorization == \"vdn\":\n",
    "            q_tot = q_out1[ac_1] + q_out2[ac_2]\n",
    "\n",
    "        error = Gt - q_tot\n",
    "        if factorization == \"owqmix\":\n",
    "            w = th.where(error > 0, th.ones_like(error), th.ones_like(error)*weight)\n",
    "            loss +=  (w * error) ** 2     \n",
    "\n",
    "        elif factorization == \"lqmix\":\n",
    "            joint_ac, joint_greedy_ac = th.cat((ac_1.unsqueeze(0), ac_2.unsqueeze(0))), th.cat((q_out1.max(0)[1].long().unsqueeze(0), q_out2.max(0)[1].long().unsqueeze(0)))\n",
    "            joint_ac_onehot = transfrom_onehot(joint_ac)\n",
    "            inputs = th.cat((q_in, joint_ac_onehot))\n",
    "            r, sigma = reward_predictor(inputs)\n",
    "            is_greedy = th.where(joint_ac == joint_greedy_ac, True, False).min()\n",
    "            temp = look_up_temp(visition, joint_ac)\n",
    "            random_z = th.rand_like(temp)\n",
    "            leniency = 1 - e**(-2*temp)\n",
    "            not_forgive = random_z > leniency\n",
    "            tot_forgive = not_forgive.min(0)[0].unsqueeze(-1).detach()\n",
    "            loss += ((r - Gt)**2/(2*sigma**2) + 0.5*th.log(sigma**2)).sum()\n",
    "            is_sr = th.where((abs(sigma) > sigma_threshold) & tot_forgive, th.ones_like(r), th.zeros_like(r))\n",
    "            min_h = look_up_h(h, joint_ac, Gt, is_sr, r, is_greedy).min()\n",
    "\n",
    "            new_Gt = th.where((is_sr==1) & ~is_greedy, min_h, Gt)   \n",
    "            new_Gt = th.where(((Gt < min_h) & ~is_greedy), min_h, new_Gt) \n",
    "            error = new_Gt - q_tot\n",
    "            loss += ((error) ** 2).sum()\n",
    "            \n",
    "        else:\n",
    "            loss += (error) ** 2       \n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss = loss/batch_size\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if ep % 500 == 0:\n",
    "        print(\"episode:\",ep)\n",
    "\n",
    "        for x in range(mat_size):\n",
    "            for y in range(mat_size):\n",
    "                if factorization in [\"qmix\",\"owqmix\", \"lqmix\"]:\n",
    "                    Q_matrix[x][y] = mixer(q_in, q_out1[x], q_out2[y])\n",
    "                elif factorization == \"vdn\":\n",
    "                    Q_matrix[x][y] = q_out1[x] + q_out2[y]\n",
    "        \n",
    "        print(\"Greedy Action:\", q_out1.max(0)[1].item()+1, q_out2.max(0)[1].item()+1)\n",
    "        print(\"Joint Action Value\\n\", Q_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19077328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
